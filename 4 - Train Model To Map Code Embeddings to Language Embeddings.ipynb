{"cells":[{"cell_type":"markdown","metadata":{"id":"6c4qza6aYpWU"},"source":["### Prerequisites\n","\n","You should have completed steps 1-3 of this tutorial before beginning this exercise.  The files required for this notebook are generated by those previous steps.\n","\n","This notebook takes approximately 3 hours to run on an AWS `p3.8xlarge` instance. "]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q8zHeBXhZHOM","executionInfo":{"status":"ok","timestamp":1671984995879,"user_tz":-360,"elapsed":21707,"user":{"displayName":"MD. RABIUL ISLAM","userId":"12721797058271486851"}},"outputId":"a76b27c0-2439-472f-e96a-0401feaf5699"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/Automate/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Yt4tPPPZQHg","executionInfo":{"status":"ok","timestamp":1671984995880,"user_tz":-360,"elapsed":9,"user":{"displayName":"MD. RABIUL ISLAM","userId":"12721797058271486851"}},"outputId":"a718e57c-8a88-4bea-f6ce-a7838b7d7378"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Automate\n"]}]},{"cell_type":"code","source":["!pip install annoy==1.11.5"],"metadata":{"id":"RuEjp4YVZpVx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671983774277,"user_tz":-360,"elapsed":10521,"user":{"displayName":"MD. RABIUL ISLAM","userId":"12721797058271486851"}},"outputId":"6825f85c-6fb7-4dab-aaf3-ff10ff7d21ca"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting annoy==1.11.5\n","  Downloading annoy-1.11.5.tar.gz (632 kB)\n","\u001b[K     |████████████████████████████████| 632 kB 28.1 MB/s \n","\u001b[?25hBuilding wheels for collected packages: annoy\n","  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for annoy: filename=annoy-1.11.5-cp38-cp38-linux_x86_64.whl size=303177 sha256=d4f2e6de5d5f928bd38db63535a7e67065036fc54d5409d3ee4ad7d84c59b24d\n","  Stored in directory: /root/.cache/pip/wheels/42/08/67/145506e4a49c72863367f7c4c2706e8e3da0841d211ddc470d\n","Successfully built annoy\n","Installing collected packages: annoy\n","Successfully installed annoy-1.11.5\n"]}]},{"cell_type":"code","source":["!pip install pathos"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q7fDNF9EoIsL","executionInfo":{"status":"ok","timestamp":1671983777691,"user_tz":-360,"elapsed":3426,"user":{"displayName":"MD. RABIUL ISLAM","userId":"12721797058271486851"}},"outputId":"34383149-affb-4ae9-f2c0-85024bbbae5e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pathos\n","  Downloading pathos-0.3.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 6.4 MB/s \n","\u001b[?25hRequirement already satisfied: dill>=0.3.6 in /usr/local/lib/python3.8/dist-packages (from pathos) (0.3.6)\n","Collecting ppft>=1.7.6.6\n","  Downloading ppft-1.7.6.6-py3-none-any.whl (52 kB)\n","\u001b[K     |████████████████████████████████| 52 kB 1.7 MB/s \n","\u001b[?25hCollecting multiprocess>=0.70.14\n","  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 71.1 MB/s \n","\u001b[?25hCollecting pox>=0.3.2\n","  Downloading pox-0.3.2-py3-none-any.whl (29 kB)\n","Installing collected packages: ppft, pox, multiprocess, pathos\n","Successfully installed multiprocess-0.70.14 pathos-0.3.0 pox-0.3.2 ppft-1.7.6.6\n"]}]},{"cell_type":"code","source":["!pip install textacy==0.6.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZN2g62eyoaTp","executionInfo":{"status":"ok","timestamp":1671983788057,"user_tz":-360,"elapsed":10371,"user":{"displayName":"MD. RABIUL ISLAM","userId":"12721797058271486851"}},"outputId":"36d86630-743d-4c2d-b602-05584c261d49"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting textacy==0.6.1\n","  Downloading textacy-0.6.1-py2.py3-none-any.whl (137 kB)\n","\u001b[K     |████████████████████████████████| 137 kB 30.6 MB/s \n","\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.8/dist-packages (from textacy==0.6.1) (1.7.3)\n","Collecting python-levenshtein>=0.12.0\n","  Downloading python_Levenshtein-0.20.8-py3-none-any.whl (9.4 kB)\n","Collecting pyphen>=0.9.4\n","  Downloading pyphen-0.13.2-py3-none-any.whl (2.0 MB)\n","\u001b[K     |████████████████████████████████| 2.0 MB 46.4 MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.17.0 in /usr/local/lib/python3.8/dist-packages (from textacy==0.6.1) (1.0.2)\n","Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.8/dist-packages (from textacy==0.6.1) (2.8.8)\n","Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from textacy==0.6.1) (5.2.0)\n","Collecting unidecode>=0.04.19\n","  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n","\u001b[K     |████████████████████████████████| 235 kB 78.2 MB/s \n","\u001b[?25hCollecting cytoolz>=0.8.0\n","  Downloading cytoolz-0.12.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n","\u001b[K     |████████████████████████████████| 1.8 MB 71.5 MB/s \n","\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from textacy==0.6.1) (1.21.6)\n","Collecting ftfy<5.0.0,>=4.2.0\n","  Downloading ftfy-4.4.3.tar.gz (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 8.8 MB/s \n","\u001b[?25hCollecting ijson>=2.3\n","  Downloading ijson-3.1.4-cp38-cp38-manylinux2010_x86_64.whl (133 kB)\n","\u001b[K     |████████████████████████████████| 133 kB 70.7 MB/s \n","\u001b[?25hRequirement already satisfied: spacy>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from textacy==0.6.1) (3.4.4)\n","Requirement already satisfied: pyemd>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from textacy==0.6.1) (0.5.1)\n","Requirement already satisfied: tqdm>=4.11.1 in /usr/local/lib/python3.8/dist-packages (from textacy==0.6.1) (4.64.1)\n","Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.8/dist-packages (from textacy==0.6.1) (2.23.0)\n","Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from cytoolz>=0.8.0->textacy==0.6.1) (0.12.0)\n","Requirement already satisfied: html5lib in /usr/local/lib/python3.8/dist-packages (from ftfy<5.0.0,>=4.2.0->textacy==0.6.1) (1.0.1)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from ftfy<5.0.0,>=4.2.0->textacy==0.6.1) (0.2.5)\n","Collecting Levenshtein==0.20.8\n","  Downloading Levenshtein-0.20.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (174 kB)\n","\u001b[K     |████████████████████████████████| 174 kB 68.6 MB/s \n","\u001b[?25hCollecting rapidfuzz<3.0.0,>=2.3.0\n","  Downloading rapidfuzz-2.13.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n","\u001b[K     |████████████████████████████████| 2.2 MB 58.1 MB/s \n","\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.10.0->textacy==0.6.1) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.10.0->textacy==0.6.1) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.10.0->textacy==0.6.1) (2022.12.7)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.10.0->textacy==0.6.1) (1.24.3)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.17.0->textacy==0.6.1) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.17.0->textacy==0.6.1) (1.2.0)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.0->textacy==0.6.1) (2.4.5)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.0->textacy==0.6.1) (2.0.8)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.0->textacy==0.6.1) (3.3.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.0->textacy==0.6.1) (1.0.9)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.0->textacy==0.6.1) (8.1.5)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.0->textacy==0.6.1) (1.10.2)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.0->textacy==0.6.1) (1.0.4)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.0->textacy==0.6.1) (6.3.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.0->textacy==0.6.1) (57.4.0)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.0->textacy==0.6.1) (0.7.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.0->textacy==0.6.1) (21.3)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.0->textacy==0.6.1) (0.10.1)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.0->textacy==0.6.1) (2.0.7)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.0->textacy==0.6.1) (0.10.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.0->textacy==0.6.1) (3.0.10)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.0->textacy==0.6.1) (2.11.3)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy>=2.0.0->textacy==0.6.1) (3.0.8)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy>=2.0.0->textacy==0.6.1) (3.0.9)\n","Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy>=2.0.0->textacy==0.6.1) (4.4.0)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=2.0.0->textacy==0.6.1) (0.7.9)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=2.0.0->textacy==0.6.1) (0.0.3)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy>=2.0.0->textacy==0.6.1) (7.1.2)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.8/dist-packages (from html5lib->ftfy<5.0.0,>=4.2.0->textacy==0.6.1) (0.5.1)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from html5lib->ftfy<5.0.0,>=4.2.0->textacy==0.6.1) (1.15.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy>=2.0.0->textacy==0.6.1) (2.0.1)\n","Building wheels for collected packages: ftfy\n","  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ftfy: filename=ftfy-4.4.3-py3-none-any.whl size=41082 sha256=f9e0ae2e81acb8f84bd49186684f8851d34983b92cf26f681dab190baa4fa0f0\n","  Stored in directory: /root/.cache/pip/wheels/97/ea/83/0ea1632b87aa8a3ecdc2c3914d7dc786ed2ff37eb9e9242f3a\n","Successfully built ftfy\n","Installing collected packages: rapidfuzz, Levenshtein, unidecode, python-levenshtein, pyphen, ijson, ftfy, cytoolz, textacy\n","Successfully installed Levenshtein-0.20.8 cytoolz-0.12.1 ftfy-4.4.3 ijson-3.1.4 pyphen-0.13.2 python-levenshtein-0.20.8 rapidfuzz-2.13.7 textacy-0.6.1 unidecode-1.3.6\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dYBqkfrrYpWb"},"outputs":[],"source":["# # Optional: you can set what GPU you want to use in a notebook like this.  \n","# # Useful if you want to run concurrent experiments at the same time on different GPUs.\n","# import os\n","# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n","# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""]},{"cell_type":"code","execution_count":6,"metadata":{"id":"rILM7he9YpWe","executionInfo":{"status":"ok","timestamp":1671983792425,"user_tz":-360,"elapsed":4373,"user":{"displayName":"MD. RABIUL ISLAM","userId":"12721797058271486851"}}},"outputs":[],"source":["from pathlib import Path\n","import numpy as np\n","from seq2seq_utils import extract_encoder_model, load_encoder_inputs\n","from keras.layers import Input, Dense, BatchNormalization, Dropout, Lambda\n","\n","from keras.models import load_model, Model\n","from seq2seq_utils import load_text_processor\n","\n","\n","#where you will save artifacts from this step\n","OUTPUT_PATH = Path('./data/code2emb/')\n","OUTPUT_PATH.mkdir(exist_ok=True)\n","\n","# These are where the artifacts are stored from steps 2 and 3, respectively.\n","seq2seq_path = Path('./data/seq2seq/')\n","langemb_path = Path('./data/lang_model/')\n","\n","# set seeds\n","from numpy.random import seed\n","seed(1)\n","import tensorflow\n","tensorflow.random.set_seed(2)"]},{"cell_type":"markdown","metadata":{"id":"ZBtQgJuFYpWf"},"source":["# Train Model That Maps Code To Sentence Embedding Space\n","\n","In step 2, we trained a seq2seq model that can summarize function code using `(code, docstring)` pairs as the training data.  \n","\n","In this step, we will fine tune the encoder from the seq2seq model to generate code embeddings in the docstring space by using `(code, docstring-embeddings)` as the training data.  Therefore, this notebook will go through the following steps:\n","\n","1. Load the seq2seq model and extract the encoder (remember seq2seq models have an encoder and a decoder).\n","2. Freeze the weights of the encoder.\n","3. Add some dense layers on top of the encoder.\n","4. Train this new model supplying by supplying `(code, docstring-embeddings)` pairs.  We will call this model `code2emb_model`.\n","5. Unfreeze the entire model, and resume training.  This helps fine tune the model a little more towards this task.\n","6. Encode all of the code, including code that does not contain a docstring and save that into a search index for future use.  "]},{"cell_type":"markdown","metadata":{"id":"O6y817CeYpWg"},"source":["### Load seq2seq model from Step 2 and extract the encoder"]},{"cell_type":"markdown","metadata":{"id":"quaXHT3XYpWh"},"source":["First load the seq2seq model from Step2, then extract the encoder (we do not need the decoder)."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2f5rpzH_YpWh","executionInfo":{"status":"ok","timestamp":1671983808966,"user_tz":-360,"elapsed":16548,"user":{"displayName":"MD. RABIUL ISLAM","userId":"12721797058271486851"}},"outputId":"e4365ee4-364e-435d-c1b8-2728fb67f4a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of encoder input: (139472, 55)\n"]}],"source":["# load the pre-processed data for the encoder (we don't care about the decoder in this step)\n","encoder_input_data, doc_length = load_encoder_inputs(seq2seq_path/'py_t_code_vecs_v2.npy')\n","seq2seq_Model = load_model(seq2seq_path/'code_summary_seq2seq_model.h5')"]},{"cell_type":"code","source":["encoder_input_data.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"onokYxqF6WqH","executionInfo":{"status":"ok","timestamp":1671983808967,"user_tz":-360,"elapsed":40,"user":{"displayName":"MD. RABIUL ISLAM","userId":"12721797058271486851"}},"outputId":"949d7961-ab47-4d7c-b7b5-cd21636d6f59"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(139472, 55)"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C36qG7vwYpWj","executionInfo":{"status":"ok","timestamp":1671983808968,"user_tz":-360,"elapsed":38,"user":{"displayName":"MD. RABIUL ISLAM","userId":"12721797058271486851"}},"outputId":"cf367ec0-a475-423e-cdd1-05bc9b763b6c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"Encoder-Model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," Encoder-Input (InputLayer)  [(None, 55)]              0         \n","                                                                 \n"," Body-Word-Embedding (Embedd  (None, 55, 800)          16001600  \n"," ing)                                                            \n","                                                                 \n"," Encoder-Batchnorm-1 (BatchN  (None, 55, 800)          3200      \n"," ormalization)                                                   \n","                                                                 \n"," Encoder-Last-GRU (GRU)      [(None, 1000),            5406000   \n","                              (None, 1000)]                      \n","                                                                 \n","=================================================================\n","Total params: 21,410,800\n","Trainable params: 21,409,200\n","Non-trainable params: 1,600\n","_________________________________________________________________\n"]}],"source":["# Extract Encoder from seq2seq model\n","encoder_model = extract_encoder_model(seq2seq_Model)\n","# Get a summary of the encoder and its layers\n","encoder_model.summary()"]},{"cell_type":"markdown","metadata":{"id":"mpeJu_pbYpWk"},"source":["Freeze the encoder"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uQojChLuYpWk","executionInfo":{"status":"ok","timestamp":1671983942783,"user_tz":-360,"elapsed":1298,"user":{"displayName":"MD. RABIUL ISLAM","userId":"12721797058271486851"}},"outputId":"9d4a181e-c4ed-40ee-909c-3198189657df"},"outputs":[{"output_type":"stream","name":"stdout","text":["<keras.engine.input_layer.InputLayer object at 0x7faccd19e8b0> False\n","<keras.layers.core.embedding.Embedding object at 0x7faccd19e9a0> False\n","<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7faccd19ecd0> False\n","<keras.layers.rnn.gru.GRU object at 0x7faccd19ef70> False\n"]}],"source":["# Freeze Encoder Model\n","for l in encoder_model.layers:\n","    l.trainable = False\n","    print(l, l.trainable)"]},{"cell_type":"markdown","metadata":{"id":"Y09zxNcrYpWl"},"source":["### Load Docstring Embeddings From From Step 3\n","\n","The target for our `code2emb` model will be docstring-embeddings instead of docstrings.  Therefore, we will use the embeddings for docstrings that we computed in step 3.  For this tutorial, we will use the average over all hidden states, which is saved in the file `avg_emb_dim500_v2.npy`.\n","\n","Note that in our experiments, a concatenation of the average, max, and last hidden state worked better than using the average alone.  However, in the interest of simplicity we demonstrate just using the average hidden state.  We leave it as an exercise to the reader to experiment with other approaches. "]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jS67mfgWYpWm","executionInfo":{"status":"ok","timestamp":1671983946357,"user_tz":-360,"elapsed":1007,"user":{"displayName":"MD. RABIUL ISLAM","userId":"12721797058271486851"}},"outputId":"0b4656c7-031b-4f91-f95c-8a15498f268a"},"outputs":[{"output_type":"stream","name":"stdout","text":["(139472, 55)\n","(139472, 400)\n"]},{"output_type":"execute_result","data":{"text/plain":["(139472, 400)"]},"metadata":{},"execution_count":21}],"source":["# Load Fitlam Embeddings\n","fastailm_emb = np.load(langemb_path/'avg_emb_dim500_v2.npy')\n","print(encoder_input_data.shape)\n","print(fastailm_emb.shape)\n","\n","# check that the encoder inputs have the same number of rows as the docstring embeddings\n","assert encoder_input_data.shape[0] == fastailm_emb.shape[0]\n","fastailm_emb.shape\n"]},{"cell_type":"markdown","metadata":{"id":"Sj6OhpI2YpWm"},"source":["### Construct `code2emb` Model Architecture\n","\n","The `code2emb` model is the encoder from the seq2seq model with some dense layers added on top.  The output of the last dense layer of this model needs to match the dimensionality of the docstring embedding, which is 500 in this case."]},{"cell_type":"code","execution_count":22,"metadata":{"id":"tS9Tsub2YpWn","executionInfo":{"status":"ok","timestamp":1671983949563,"user_tz":-360,"elapsed":422,"user":{"displayName":"MD. RABIUL ISLAM","userId":"12721797058271486851"}}},"outputs":[],"source":["#### Encoder Model ####\n","encoder_inputs = Input(shape=(doc_length,), name='Encoder-Input')\n","enc_out = encoder_model(encoder_inputs)\n","\n","# first dense layer with batch norm\n","x = Dense(400, activation='relu')(enc_out)\n","x = BatchNormalization(name='bn-1')(x)\n","out = Dense(400)(x)\n","code2emb_model = Model([encoder_inputs], out)"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"d0VsvRXwYpWo","outputId":"d64719fb-bd13-405f-d289-207450f60664","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671983951305,"user_tz":-360,"elapsed":9,"user":{"displayName":"MD. RABIUL ISLAM","userId":"12721797058271486851"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," Encoder-Input (InputLayer)  [(None, 55)]              0         \n","                                                                 \n"," Encoder-Model (Functional)  (None, 1000)              21410800  \n","                                                                 \n"," dense_2 (Dense)             (None, 400)               400400    \n","                                                                 \n"," bn-1 (BatchNormalization)   (None, 400)               1600      \n","                                                                 \n"," dense_3 (Dense)             (None, 400)               160400    \n","                                                                 \n","=================================================================\n","Total params: 21,973,200\n","Trainable params: 561,600\n","Non-trainable params: 21,411,600\n","_________________________________________________________________\n"]}],"source":["code2emb_model.summary()"]},{"cell_type":"markdown","metadata":{"id":"TR_ElKg4YpWo"},"source":["### Train the `code2emb` Model\n","\n","The model we are training is relatively simple - with two dense layers on top of the pre-trained encoder.  We are leaving the encoder frozen at first, then will unfreeze the encoder in a later step."]},{"cell_type":"code","execution_count":24,"metadata":{"id":"1UERxkRFYpWo","outputId":"710493c2-2238-4957-e910-61028530cbe2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671984279102,"user_tz":-360,"elapsed":324651,"user":{"displayName":"MD. RABIUL ISLAM","userId":"12721797058271486851"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/nadam.py:78: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(Nadam, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","82/82 [==============================] - 34s 336ms/step - loss: -0.6147 - val_loss: -0.6605\n","Epoch 2/10\n","82/82 [==============================] - 28s 343ms/step - loss: -0.6913 - val_loss: -0.6856\n","Epoch 3/10\n","82/82 [==============================] - 28s 340ms/step - loss: -0.6932 - val_loss: -0.6867\n","Epoch 4/10\n","82/82 [==============================] - 27s 332ms/step - loss: -0.6941 - val_loss: -0.6871\n","Epoch 5/10\n","82/82 [==============================] - 27s 335ms/step - loss: -0.6946 - val_loss: -0.6875\n","Epoch 6/10\n","82/82 [==============================] - 28s 338ms/step - loss: -0.6952 - val_loss: -0.6880\n","Epoch 7/10\n","82/82 [==============================] - 27s 332ms/step - loss: -0.6956 - val_loss: -0.6879\n","Epoch 8/10\n","82/82 [==============================] - 27s 335ms/step - loss: -0.6959 - val_loss: -0.6884\n","Epoch 9/10\n","82/82 [==============================] - 27s 333ms/step - loss: -0.6961 - val_loss: -0.6878\n","Epoch 10/10\n","82/82 [==============================] - 27s 334ms/step - loss: -0.6964 - val_loss: -0.6881\n"]}],"source":["from keras.callbacks import CSVLogger, ModelCheckpoint\n","from keras import optimizers\n","import tensorflow.compat.v1 as tf\n","\n","code2emb_model.compile(optimizer=optimizers.Nadam(lr=0.002), loss=tf.keras.losses.cosine_proximity)\n","script_name_base = 'code2emb_model_'\n","csv_logger = CSVLogger('{:}.log'.format(script_name_base))\n","model_checkpoint = ModelCheckpoint('{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5'.format(script_name_base),\n","                                   save_best_only=True)\n","\n","batch_size = 1500\n","epochs = 10\n","history = code2emb_model.fit([encoder_input_data], fastailm_emb,\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          validation_split=0.12, callbacks=[csv_logger, model_checkpoint])"]},{"cell_type":"code","source":["encoder_input_data.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1upziGa08GDG","executionInfo":{"status":"ok","timestamp":1671984283433,"user_tz":-360,"elapsed":410,"user":{"displayName":"MD. RABIUL ISLAM","userId":"12721797058271486851"}},"outputId":"c85788e7-f786-43f8-d71b-a178fac9ee25"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(139472, 55)"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"kvckQpTRYpWp"},"source":["`.7453`"]},{"cell_type":"markdown","metadata":{"id":"605D_rogYpWp"},"source":["### Unfreeze all Layers of Model and Resume Training"]},{"cell_type":"markdown","metadata":{"id":"U_t84yaYYpWq"},"source":["In the previous step, we left the encoder frozen.  Now that the dense layers are trained, we will unfreeze the entire model and let it train some more.  This will hopefully allow this model to specialize on this task a bit more."]},{"cell_type":"code","execution_count":26,"metadata":{"id":"wkDwyNKjYpWq","outputId":"0537408d-9bee-4aa9-8e3e-3acd20f46c26","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671984286649,"user_tz":-360,"elapsed":400,"user":{"displayName":"MD. RABIUL ISLAM","userId":"12721797058271486851"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["<keras.engine.input_layer.InputLayer object at 0x7fac6662b0d0> True\n","<keras.engine.functional.Functional object at 0x7faccd19e790> True\n","<keras.layers.core.dense.Dense object at 0x7fac6662b4c0> True\n","<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x7fad3bc13ac0> True\n","<keras.layers.core.dense.Dense object at 0x7fac6662b130> True\n"]}],"source":["for l in code2emb_model.layers:\n","    l.trainable = True\n","    print(l, l.trainable)"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"jUAH0ppvYpWv","executionInfo":{"status":"ok","timestamp":1671984293022,"user_tz":-360,"elapsed":421,"user":{"displayName":"MD. RABIUL ISLAM","userId":"12721797058271486851"}}},"outputs":[],"source":["code2emb_model.compile(optimizer=optimizers.Nadam(lr=0.0001), loss=tf.keras.losses.cosine_proximity)\n","script_name_base = 'code2emb_model_unfreeze_'\n","csv_logger = CSVLogger('{:}.log'.format(script_name_base))\n","model_checkpoint = ModelCheckpoint('{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5'.format(script_name_base),\n","                                   save_best_only=True)\n","\n","batch_size = 100\n","epochs = 10\n","history = code2emb_model.fit([encoder_input_data], fastailm_emb,\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          initial_epoch=16,\n","          validation_split=0.12, callbacks=[csv_logger, model_checkpoint])"]},{"cell_type":"markdown","metadata":{"id":"iPYmLKc8YpWw"},"source":["### Save `code2emb` model"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"OmVNvhdxYpWx","executionInfo":{"status":"ok","timestamp":1671984299387,"user_tz":-360,"elapsed":3,"user":{"displayName":"MD. RABIUL ISLAM","userId":"12721797058271486851"}}},"outputs":[],"source":["code2emb_model.save(OUTPUT_PATH/'code2emb_model.hdf5')"]},{"cell_type":"markdown","metadata":{"id":"YTeYZcLkYpWz"},"source":["# Vectorize all of the code without docstrings\n","\n","We want to vectorize all of the code without docstrings so we can test the efficacy of the search on the code that was never seen by the model. "]},{"cell_type":"code","execution_count":29,"metadata":{"id":"LX9JwOy5YpWz","executionInfo":{"status":"ok","timestamp":1671984302907,"user_tz":-360,"elapsed":374,"user":{"displayName":"MD. RABIUL ISLAM","userId":"12721797058271486851"}}},"outputs":[],"source":["from keras.models import load_model\n","from pathlib import Path\n","import numpy as np\n","from seq2seq_utils import load_text_processor\n","code2emb_path = Path('./data/code2emb/')\n","seq2seq_path = Path('./data/seq2seq/')\n","data_path = Path('./data/processed_data/')"]},{"cell_type":"code","source":["!pip install joblib"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pu3SH_ZlpANh","executionInfo":{"status":"ok","timestamp":1671984326223,"user_tz":-360,"elapsed":3109,"user":{"displayName":"MD. RABIUL ISLAM","userId":"12721797058271486851"}},"outputId":"e2237bdb-ee08-4f02-cf88-2aff6346cb3b"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (1.2.0)\n"]}]},{"cell_type":"code","source":["!pip install keras==2.9.0 "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EqieBQC7pKKi","executionInfo":{"status":"ok","timestamp":1671983644168,"user_tz":-360,"elapsed":7854,"user":{"displayName":"MD. RABIUL ISLAM","userId":"12721797058271486851"}},"outputId":"54541a52-d9f8-479a-8c7b-615ee3a2c5c7"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting keras==2.9.0\n","  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 13.7 MB/s \n","\u001b[?25hInstalling collected packages: keras\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.11.0\n","    Uninstalling keras-2.11.0:\n","      Successfully uninstalled keras-2.11.0\n","Successfully installed keras-2.9.0\n"]}]},{"cell_type":"markdown","source":["pad_sequences error\n","\n","````from keras_preprocessing.sequence import pad_sequences````"],"metadata":{"id":"Mc1vDCTlrXI5"}},{"cell_type":"code","execution_count":32,"metadata":{"id":"CU9qxhJXYpW0","outputId":"53811e43-e095-49b5-ac33-dfaa7f7ad077","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671984333564,"user_tz":-360,"elapsed":4202,"user":{"displayName":"MD. RABIUL ISLAM","userId":"12721797058271486851"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Size of vocabulary for data/seq2seq/py_code_proc_v2.dpkl: 20,002\n"]}],"source":["code2emb_model = load_model(code2emb_path/'code2emb_model.hdf5')\n","num_encoder_tokens, enc_pp = load_text_processor(seq2seq_path/'py_code_proc_v2.dpkl')\n","\n","with open(data_path/'without_docstrings.function', 'r') as f:\n","    no_docstring_funcs = f.readlines()"]},{"cell_type":"markdown","metadata":{"id":"z1rGcOexYpW4"},"source":["### Pre-process code without docstrings for input into `code2emb` model\n","\n","We use the same transformer we used to train the original model."]},{"cell_type":"code","execution_count":33,"metadata":{"id":"wAsiqiK2YpW7","outputId":"e44df544-e26c-4975-da2e-c478ab5df34f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671984339366,"user_tz":-360,"elapsed":406,"user":{"displayName":"MD. RABIUL ISLAM","userId":"12721797058271486851"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['function_tokens\\n',\n"," 'def __init__ self leafs edges self edges edges self leafs sorted leafs\\n',\n"," 'def __eq__ self other if isinstance other Node return id self id other or self leafs other leafs and self edges other edges else return False\\n',\n"," 'def __repr__ self return Node leafs edges format self leafs self edges\\n',\n"," 'staticmethod def _isCapitalized token return len token 1 and token isalpha and token 0 isupper and token 1 islower\\n']"]},"metadata":{},"execution_count":33}],"source":["# tokenized functions that did not contain docstrigns\n","no_docstring_funcs[:5]"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"haCJNcLoYpW8","outputId":"1d269ad0-ed62-4c8d-e650-8566a8cc75db","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671984635791,"user_tz":-360,"elapsed":294102,"user":{"displayName":"MD. RABIUL ISLAM","userId":"12721797058271486851"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:root:...tokenizing data\n","WARNING:root:...indexing data\n","WARNING:root:...padding data\n"]}],"source":["encinp = enc_pp.transform_parallel(no_docstring_funcs)\n","np.save(code2emb_path/'nodoc_encinp.npy', encinp)"]},{"cell_type":"markdown","metadata":{"id":"HVJljbykYpW-"},"source":["### Extract code vectors"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"zGw5pc2MYpXA","executionInfo":{"status":"ok","timestamp":1671985010449,"user_tz":-360,"elapsed":7482,"user":{"displayName":"MD. RABIUL ISLAM","userId":"12721797058271486851"}}},"outputs":[],"source":["from keras.models import load_model\n","from pathlib import Path\n","import numpy as np\n","code2emb_path = Path('./data/code2emb/')\n","encinp = np.load(code2emb_path/'nodoc_encinp.npy')\n","code2emb_model = load_model(code2emb_path/'code2emb_model.hdf5')"]},{"cell_type":"markdown","metadata":{"id":"KP-CBwsoYpXB"},"source":["Use the `code2emb` model to map the code into the same vector space as natural language "]},{"cell_type":"code","execution_count":6,"metadata":{"id":"YulmH1BvYpXD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671985370805,"user_tz":-360,"elapsed":357255,"user":{"displayName":"MD. RABIUL ISLAM","userId":"12721797058271486851"}},"outputId":"bfe7b10b-09de-43e2-b83a-eff9b8a0de9d"},"outputs":[{"output_type":"stream","name":"stdout","text":["46456/46456 [==============================] - 347s 7ms/step\n"]}],"source":["nodoc_vecs = code2emb_model.predict(encinp, batch_size=10)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"dryLlGmdYpXE","executionInfo":{"status":"ok","timestamp":1671985381025,"user_tz":-360,"elapsed":430,"user":{"displayName":"MD. RABIUL ISLAM","userId":"12721797058271486851"}}},"outputs":[],"source":["# make sure the number of output rows equal the number of input rows\n","assert nodoc_vecs.shape[0] == encinp.shape[0]"]},{"cell_type":"markdown","metadata":{"id":"gDeGRm_KYpXE"},"source":["Save the vectorized code"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"JaWA3UBaYpXG","executionInfo":{"status":"ok","timestamp":1671985387488,"user_tz":-360,"elapsed":3705,"user":{"displayName":"MD. RABIUL ISLAM","userId":"12721797058271486851"}}},"outputs":[],"source":["np.save(code2emb_path/'nodoc_vecs.npy', nodoc_vecs)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"colab":{"provenance":[]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}